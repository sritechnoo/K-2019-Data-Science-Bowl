{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "You might have noticed that the train dataset is composed of over 11M data points, but there are only 17k training labels, and 1000k test labels you are predicting. The reason for that is there are many thousand different entries for each `installation_id`, each representing an `event`. This notebook simply gathers all the events into 17k groups, each group corresponds to an `installation_id`. Then, it takes the aggregation (using sums, counts, mean, std, etc.) of those groups, thus resulting in a dataset of summary statistics of each `installation_id`. After that, it simply fits a model on that dataset.\n",
    "\n",
    "## Updates\n",
    "\n",
    "V20:\n",
    "* Updated variable names for clarity.\n",
    "\n",
    "V17:\n",
    "* Removed statistics on event codes, since that created a lot of columns and LGBM seems to overfit on that information.\n",
    "\n",
    "V16:\n",
    "* Added mode of title `accuracy_group` (retrieved from training set) as a feature\n",
    "\n",
    "V10:\n",
    "* Fixed labelling problem. Before that, I was blindly predicting the target without even the title I was trying to assess ðŸ¤¦. I added that now by using the \"title\" column from `train_labels.csv`, and using the last row of each installation_id from `test.csv` to construct a `test_labels` dataframe.\n",
    "\n",
    "V8: \n",
    "* Added `cv_train`, a function that trains k-models on each of k-fold CV splits. Then, you can use function `cv_predict` to use the list of models to predict an output (and blend the results).\n",
    "* Added more summary statistics for `event_code` and `game_time`, including skewness of the distribution.\n",
    "\n",
    "## References\n",
    "\n",
    "* CV idea inspired from [this kernel](https://www.kaggle.com/tanreinama/ds-bowl-2019-simple-lgbm-aggregated-data-with-cv). Thank you!\n",
    "* Adding mode as a feature: https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import lightgbm as lgb\n",
    "import scipy as sp\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Only load those columns in order to save space\n",
    "keep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n",
    "\n",
    "train = pd.read_csv(r'D:\\Artificial Intelligence\\Kaggle\\2019 Data Science Bowl\\Data\\train.csv')\n",
    "train_labels = pd.read_csv(r'D:\\Artificial Intelligence\\Kaggle\\2019 Data Science Bowl\\Data\\train_labels.csv')\n",
    "spec = pd.read_csv(r'D:\\Artificial Intelligence\\Kaggle\\2019 Data Science Bowl\\Data\\specs.csv')\n",
    "    \n",
    "test = pd.read_csv(r'D:\\Artificial Intelligence\\Kaggle\\2019 Data Science Bowl\\Data\\test.csv')\n",
    "submission = pd.read_csv(r'D:\\Artificial Intelligence\\Kaggle\\2019 Data Science Bowl\\Data\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assess = test[test.type == 'Assessment'].copy()\n",
    "test_labels = submission.copy()\n",
    "test_labels['title'] = test_labels.installation_id.progress_apply(\n",
    "    lambda install_id: test_assess[test_assess.installation_id == install_id].iloc[-1].title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group and Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_game_time_stats(group, col):\n",
    "    return group[\n",
    "        ['installation_id', col, 'event_count', 'game_time']\n",
    "    ].groupby(['installation_id', col]).agg(\n",
    "        [np.mean, np.sum, np.std]\n",
    "    ).reset_index().pivot(\n",
    "        columns=col,\n",
    "        index='installation_id'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_reduce(df, df_labels):\n",
    "    \"\"\"\n",
    "    Author: https://www.kaggle.com/xhlulu/\n",
    "    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n",
    "    \"\"\"\n",
    "    \n",
    "    # First only filter the useful part of the df\n",
    "    df = df[df.installation_id.isin(df_labels.installation_id.unique())]\n",
    "    \n",
    "    # group1 is am intermediary \"game session\" group,\n",
    "    # which are reduced to one record by game session. group_game_time takes\n",
    "    # the max value of game_time (final game time in a session) and \n",
    "    # of event_count (total number of events happened in the session).\n",
    "    group_game_time = df.drop(columns=['event_id', 'event_code']).groupby(\n",
    "        ['game_session', 'installation_id', 'title', 'type', 'world']\n",
    "    ).max().reset_index()\n",
    "\n",
    "    # group3, group4 are grouped by installation_id \n",
    "    # and reduced using summation and other summary stats\n",
    "    title_group = (\n",
    "        pd.get_dummies(\n",
    "            group_game_time.drop(columns=['game_session', 'event_count', 'game_time']),\n",
    "            columns=['title', 'type', 'world'])\n",
    "        .groupby(['installation_id'])\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    event_game_time_group = (\n",
    "        group_game_time[['installation_id', 'event_count', 'game_time']]\n",
    "        .groupby(['installation_id'])\n",
    "        .agg([np.sum, np.mean, np.std, np.min, np.max])\n",
    "    )\n",
    "    \n",
    "    # Additional stats on group1\n",
    "    world_time_stats = compute_game_time_stats(group_game_time, 'world')\n",
    "    type_time_stats = compute_game_time_stats(group_game_time, 'type')\n",
    "    \n",
    "    return (\n",
    "        title_group.join(event_game_time_group)\n",
    "        .join(world_time_stats)\n",
    "        .join(type_time_stats)\n",
    "        .fillna(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_small = group_and_reduce(train, train_labels)\n",
    "test_small = group_and_reduce(test, test_labels)\n",
    "\n",
    "print(train_small.shape)\n",
    "train_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding mode as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title_mode(train_labels):\n",
    "    titles = train_labels.title.unique()\n",
    "    title2mode = {}\n",
    "\n",
    "    for title in titles:\n",
    "        mode = (\n",
    "            train_labels[train_labels.title == title]\n",
    "            .accuracy_group\n",
    "            .value_counts()\n",
    "            .index[0]\n",
    "        )\n",
    "        title2mode[title] = mode\n",
    "    return title2mode\n",
    "\n",
    "def add_title_mode(labels, title2mode):\n",
    "    labels['title_mode'] = labels.title.apply(lambda title: title2mode[title])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2mode = create_title_mode(train_labels)\n",
    "train_labels = add_title_mode(train_labels, title2mode)\n",
    "test_labels = add_title_mode(test_labels, title2mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine train/test labels with summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(train_labels, last_records_only=True):\n",
    "    \"\"\"\n",
    "    last_records_only (bool): Use only the last record of each user.\n",
    "    \"\"\"\n",
    "    final_train = pd.get_dummies(\n",
    "        (\n",
    "            train_labels.set_index('installation_id')\n",
    "            .drop(columns=['num_correct', 'num_incorrect', 'accuracy', 'game_session'])\n",
    "            .join(train_small)\n",
    "        ), \n",
    "        columns=['title']\n",
    "    )\n",
    "    \n",
    "    if last_records_only:\n",
    "        final_train = (\n",
    "            final_train\n",
    "            .reset_index()\n",
    "            .groupby('installation_id')\n",
    "            .apply(lambda x: x.iloc[-1])\n",
    "            .drop(columns='installation_id')\n",
    "        )\n",
    "    \n",
    "    return final_train\n",
    "\n",
    "def preprocess_test(test_labels, test_small):\n",
    "    return pd.get_dummies(\n",
    "        test_labels.set_index('installation_id').join(test_small), columns=['title']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = preprocess_train(train_labels)\n",
    "print(final_train.shape)\n",
    "final_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = preprocess_test(test_labels, test_small)\n",
    "print(final_test.shape)\n",
    "final_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train(X, y, cv, **kwargs):\n",
    "    \"\"\"\n",
    "    Author: https://www.kaggle.com/xhlulu/\n",
    "    Source: https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    kf = KFold(n_splits=cv, random_state=2019)\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n",
    "        \n",
    "        train_set = lgb.Dataset(x_train, y_train)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        \n",
    "        model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], **kwargs)\n",
    "        models.append(model)\n",
    "        \n",
    "        if kwargs.get(\"verbose_eval\"):\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def cv_predict(models, X):\n",
    "    return np.mean([model.predict(X) for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_train.drop(columns='accuracy_group').values\n",
    "y = final_train['accuracy_group'].values\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'feature_fraction': 0.2,\n",
    "    'max_height': 3,\n",
    "    'lambda_l1': 10,\n",
    "    'lambda_l2': 10,\n",
    "    'metric': 'multiclass',\n",
    "    'objective': 'multiclass',\n",
    "    'num_classes': 4,\n",
    "    'random_state': 2019\n",
    "}\n",
    "\n",
    "models = cv_train(X, y, cv=20, params=params, num_boost_round=1000,\n",
    "                  early_stopping_rounds=100, verbose_eval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_test.drop(columns=['accuracy_group'])\n",
    "test_pred = cv_predict(models=models, X=X_test).argmax(axis=1)\n",
    "\n",
    "final_test['accuracy_group'] = test_pred\n",
    "final_test[['accuracy_group']].to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    lgb.plot_importance(model, max_num_features=15, height=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
