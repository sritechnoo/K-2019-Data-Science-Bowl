{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This kernel uses fastai version-2 on a fork of the original notebook [\"Fastai 2019 Data Science Bowl\"](https://www.kaggle.com/manyregression/fastai-2019-data-science-bowl), thanks to Valeriy Mukhtarulin aka @manyregression. \n",
    "\n",
    "### Since the use of internet is not allowed, I created a [fastai2](https://www.kaggle.com/sheriytm/fastai2) dataset with the required files to run fastai version-2. Add [my dataset](https://www.kaggle.com/sheriytm/fastai2), follow the installation steps in the following cells and you are good to go.\n",
    "\n",
    "### Feel free to upvote [my dataset](https://www.kaggle.com/sheriytm/fastai2) and experiment with it to your hearts content :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/fastai/fastai2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la /kaggle/input/fastai2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/fastai2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install /kaggle/working/fastai2/fastcore-0.1.6-py3-none-any.whl\n",
    "\n",
    "!pip install /kaggle/working/fastai2/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
    "\n",
    "!pip install /kaggle/working/fastai2/fastprogress-0.1.22-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from fastai.tabular import * \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = Path(\"D:\\Artificial Intelligence\\Kaggle\\2019 Data Science Bowl\\Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to 2019 DS bowl with fastai v1\n",
    "\n",
    "I used these awesome notebooks:\n",
    "    \n",
    "    https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction\n",
    "    https://www.kaggle.com/amanooo/dsb-2019-regressors-and-optimal-rounding\n",
    "    https://www.kaggle.com/tarandro/regression-with-less-overfitting\n",
    "    https://www.kaggle.com/keremt/fastai-feature-engineering-part1-6160-features\n",
    "    https://www.kaggle.com/keremt/fastai-model-part1-regression\n",
    "    https://www.kaggle.com/keremt/fastai-model-part2-upgraded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Looking at data](#Looking-at-data)\n",
    "* [Preparing data](#Preparing-data)\n",
    "* [Approach](#How-to-approach)\n",
    "* [Train](#Train)\n",
    "* [Submission](#Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = pd.read_csv(home/\"specs.csv\"); len(specs)\n",
    "specs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(home/\"train_labels.csv\", encoding='utf-8'); len(train_labels)\n",
    "train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(home/\"sample_submission.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "types = {\"event_code\": np.int16, \"event_count\": np.int16, \"game_time\": np.int32}\n",
    "raw_train = pd.read_csv(home/\"train.csv\", dtype=types)\n",
    "raw_train[\"timestamp\"] = pd.to_datetime(raw_train[\"timestamp\"]); len(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = pd.read_csv(home/\"test.csv\", dtype=types)\n",
    "raw_test[\"timestamp\"] = pd.to_datetime(raw_test[\"timestamp\"])\n",
    "raw_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `game_time` is not captured correctly - there's a huge window in between some events in a given session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_train[raw_train[\"game_session\"] == \"969a6c0d56aa4683\"].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target.\n",
    "\n",
    "We are told: The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). The outcomes in this competition are grouped into 4 groups (labeled `accuracy_group` in the data):\n",
    "\n",
    "    3: the assessment was solved on the first attempt\n",
    "    2: the assessment was solved on the second attempt\n",
    "    1: the assessment was solved after 3 or more attempts\n",
    "    0: the assessment was never solved\n",
    "\n",
    "For each installation_id represented in the test set, you must predict the accuracy_group **of the last assessment** for that installation_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are told in the data description that:\n",
    "\n",
    "* The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set.\n",
    "* Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110.\n",
    "* If the attempt was correct, it contains \"correct\":true.\n",
    "\n",
    "We also know:\n",
    "\n",
    "* The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt).\n",
    "* Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices.\n",
    "* In the training set, you are provided **the full history of gameplay data.**\n",
    "* In the test set, **we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts.**\n",
    "* Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to approach\n",
    "\n",
    "For test set we guess the group based on **single** installation_id. But the train\\test datasets contain **repeatable** installation ids with different game sessions.\n",
    "Hence it makes sense to guess the group for each assessment using history.\n",
    "\n",
    "A couple of more points:\n",
    "\n",
    "* do not use any assessment data for the current assessment prediction except title to prevent overfit\n",
    "* regression with the right coeff\n",
    "\n",
    "The questions:\n",
    "* does randomly truncated history in test conflicts with the above?\n",
    "From the test dataset, the last asssessment data is cleaned. So it looks like a session with only 1 event.\n",
    "To have a good validation dataset however, it should be the same as test - https://www.kaggle.com/tarandro/regression-with-less-overfitting\n",
    "* Is more than 1 correct submission impossible per session? Does it mean noise - two devices with the same id sharing the same session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove `installation_id` without any assesments\n",
    "ids_with_subms = raw_train[raw_train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n",
    "raw_train = pd.merge(raw_train, ids_with_subms, on=\"installation_id\", how=\"inner\"); len(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce event_id to make data preparation faster\n",
    "\n",
    "specs['hashed_info']=specs['info'].transform(hash)\n",
    "unique_specs=pd.DataFrame(specs[['hashed_info']].drop_duplicates())\n",
    "unique_specs[\"id\"] = np.arange(len(unique_specs))\n",
    "specs = pd.merge(specs,unique_specs,on='hashed_info',how='left')\n",
    "event_id_mapping = dict(zip(specs.event_id,specs.id))\n",
    "raw_train[\"event_id\"] = raw_train[\"event_id\"].map(event_id_mapping)\n",
    "raw_test[\"event_id\"] = raw_test[\"event_id\"].map(event_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(correct_data):\n",
    "    # Rounding correct > 1 to 1 lowers the score. Why?\n",
    "    correct = len(correct_data.loc[correct_data])\n",
    "    wrong = len(correct_data.loc[~correct_data])\n",
    "    accuracy = correct/(correct + wrong) if correct + wrong else 0\n",
    "    return accuracy, correct, wrong\n",
    "\n",
    "def get_group(accuracy):\n",
    "    if not accuracy:\n",
    "        return 0\n",
    "    elif accuracy == 1:\n",
    "        return 3\n",
    "    elif accuracy >= 0.5:\n",
    "        return 2\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I prefer this over calculating average\n",
    "def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(data: pd.DataFrame, one_hot: List[str], test=False) -> pd.DataFrame:\n",
    "    one_hot_dict = defaultdict(int)\n",
    "\n",
    "    prepared = []\n",
    "    for id_, g in tqdm(data.groupby(\"installation_id\", sort=False)):\n",
    "        features = process_id(g, one_hot, one_hot_dict.copy(), test)\n",
    "        if not features:\n",
    "            continue\n",
    "        if test:\n",
    "            features[-1][\"is_test\"] = 1\n",
    "        prepared.extend(features)\n",
    "    return pd.DataFrame(prepared).fillna(0).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id(id_data: pd.DataFrame, one_hot_cols, one_hot_dict, test: bool) -> pd.DataFrame:\n",
    "    a_accuracy, a_group, a_correct, a_wrong, counter, accumulated_duration_mean = 0, 0, 0, 0, 0, 0\n",
    "    a_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "    a_durations = defaultdict(int)\n",
    "    features = []\n",
    "\n",
    "    for s, gs in id_data.groupby(\"game_session\", sort=False):\n",
    "        def update_counter(counter: dict, column: str):\n",
    "            session_counter = Counter(gs.loc[:, column])\n",
    "            for value in session_counter.keys():\n",
    "                counter[f\"{column}_{value}\"] += session_counter[value]\n",
    "            return counter\n",
    "\n",
    "        def process_session(gs):\n",
    "            # share state with parent process_id()\n",
    "            nonlocal one_hot_dict, a_groups, a_durations, a_accuracy, a_group, a_correct, a_wrong, counter, accumulated_duration_mean\n",
    "            # increment one hot columns for session, e.g. Bird Measurer: 50\n",
    "            def accumulate():\n",
    "                nonlocal accumulated_duration_mean\n",
    "                # accumulated one_hot features per id for a given session, e.g. Bird Measurer: 50\n",
    "                for c in one_hot_cols:\n",
    "                    one_hot_dict.update(update_counter(one_hot_dict, c))\n",
    "                duration = (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds\n",
    "                # an accumulated session duration mean\n",
    "                accumulated_duration_mean = lin_comb(accumulated_duration_mean or duration,\n",
    "                                                     duration, beta=0.9)\n",
    "                a_durations[f\"duration_{gs.title.iloc[0]}\"] = duration\n",
    "                \n",
    "            if gs[\"type\"].iloc[0] != \"Assessment\":\n",
    "                accumulate()\n",
    "                return\n",
    "\n",
    "            guess_mask = ((gs[\"event_data\"].str.contains(\"correct\")) & \n",
    "             (((gs[\"event_code\"] == 4100) &(~gs[\"title\"].str.startswith(\"Bird\")) | \n",
    "               ((gs[\"event_code\"] == 4110) & (gs[\"title\"].str.startswith(\"Bird\"))))))\n",
    "            answers = gs.loc[guess_mask, \"event_data\"].apply(lambda x: json.loads(x).get(\"correct\"))\n",
    "\n",
    "            # skip assessments without attempts in train\n",
    "            if answers.empty and not test:\n",
    "                accumulate()\n",
    "                return\n",
    "\n",
    "            accuracy, correct, wrong = get_accuracy(answers)\n",
    "            group = get_group(accuracy)\n",
    "            processed = {\"installation_id\": id_data[\"installation_id\"].iloc[0],\n",
    "                         \"title\": gs[\"title\"].iloc[0],\n",
    "                         \"timestamp\": gs[\"timestamp\"].iloc[0],\n",
    "                         \"accumulated_duration_mean\": accumulated_duration_mean,\n",
    "                         \"accumulated_correct\": a_correct, \"accumulated_incorrect\": a_wrong,\n",
    "                         \"accumulated_accuracy_mean\": a_accuracy/counter if counter > 0 else 0,\n",
    "                         \"accumulated_accuracy_group_mean\": a_group/counter if counter > 0 else 0, \n",
    "                         \"accuracy_group\": group,\n",
    "                        }\n",
    "            processed.update(a_groups)\n",
    "            processed.update(one_hot_dict)\n",
    "            processed.update(a_durations)\n",
    "            counter += 1\n",
    "            a_accuracy += accuracy\n",
    "            a_correct += correct\n",
    "            a_wrong += wrong\n",
    "            a_group += group\n",
    "            a_groups[str(group)] += 1\n",
    "            accumulate()\n",
    "            return processed\n",
    "        \n",
    "        # skip sessions with 1 row\n",
    "        if len(gs) == 1 and not test:\n",
    "            continue\n",
    "        gs.reset_index(inplace=True, drop=True)\n",
    "        if (gs[\"timestamp\"].iloc[-1] - gs[\"timestamp\"].iloc[0]).seconds > 1800:\n",
    "            gs[\"passed\"] = gs.loc[:, \"timestamp\"].diff().apply(lambda x: x.seconds)\n",
    "            id_max = gs[\"passed\"].idxmax()\n",
    "            if gs[\"passed\"].max() > 1800:\n",
    "                session = gs.iloc[:id_max]\n",
    "                continued_session = gs.iloc[id_max:]\n",
    "                fs = process_session(session)\n",
    "                c_fs = process_session(continued_session)\n",
    "                if fs:\n",
    "                    features.append(fs)\n",
    "                if c_fs:\n",
    "                    features.append(c_fs)\n",
    "                continue\n",
    "\n",
    "        session_features = process_session(gs)\n",
    "        if session_features:\n",
    "            features.append(session_features)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_counters=[\"title\", \"type\", \"event_code\", \"event_id\"]\n",
    "train = prepare(raw_train, one_hot_counters)\n",
    "# train = prepare(raw_train.iloc[:1_000_000], one_hot_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(train, \"timestamp\", prefix=\"timestamp_\", time=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = prepare(raw_test, one_hot=one_hot_counters, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the case when one hot encoded columns don't match between datasets\n",
    "add_datepart(test, \"timestamp\", prefix=\"timestamp_\", time=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why discard good data from test, let's use all the taken assessments in train!\n",
    "train = (pd.concat([train, test[test[\"is_test\"] == 0].drop(columns=[\"is_test\"])],\n",
    "                   ignore_index=True, sort=False)).fillna(0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.loc[test[\"is_test\"] == 1].reset_index(drop=True)\n",
    "#test.drop(columns=[\"accuracy_group\", \"is_test\"], inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_title1 = train['title'].value_counts().index[0]\n",
    "session_title2 = train['title'].value_counts().index[1]\n",
    "session_title3 = train['title'].value_counts().index[2]\n",
    "session_title4 = train['title'].value_counts().index[3]\n",
    "session_title5 = train['title'].value_counts().index[4]\n",
    "\n",
    "train['title'] = train['title'].replace({session_title1:0,session_title2:1,session_title3:2,session_title4:3,session_title5:4})\n",
    "test['title'] = test['title'].replace({session_title1:0,session_title2:1,session_title3:2,session_title4:3,session_title5:4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if type(col) != str:\n",
    "        train = train.rename(columns={col:str(col)})\n",
    "        test = test.rename(columns={col:str(col)})\n",
    "\n",
    "col_order = sorted(train.columns)\n",
    "train = train.ix[:,col_order]\n",
    "test = test.ix[:,col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=\"accuracy_group\", inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = train.drop(columns=[\"accuracy_group\"]).columns.difference(test.columns)\n",
    "display(f\"Test doesn't contain {diff.values}\")\n",
    "display(f\"Train doesn't contain {test.columns.difference(train.columns).values}\")\n",
    "train.drop(columns=diff, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train = train.copy()\n",
    "# train = main_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols = [\"timestamp_Second\"]\n",
    "for col in train.columns.values:\n",
    "    counts = train[col].value_counts().iloc[0]\n",
    "    if (counts / train.shape[0]) >= 0.99:\n",
    "        del_cols.append(col)\n",
    "train.drop(columns=del_cols, inplace=True, errors=\"ignore\")\n",
    "test.drop(columns=del_cols, inplace=True, errors=\"ignore\")\n",
    "display(f\"Dropped {del_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper validation dataset\n",
    "\n",
    "Let's assume the second hidden test is the same as this one. I.e. we predict the last assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "train = train[train[train.columns[train.columns.str.startswith(\"duration_\", na=False)].to_list()].apply(sum, axis=1) < 10000].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the last assessments per id\n",
    "valid_idx = [g.iloc[-1].name for i, g in train.groupby(\"installation_id\", sort=False)]; len(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.accuracy_group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[valid_idx].accuracy_group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.title.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[valid_idx].title.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = train.columns[train.columns.str.startswith(\"timestamp_\", na=False)].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"accuracy_group\"\n",
    "cat_names = list(filter(lambda x: x not in [\"timestamp_Elapsed\"], date_cols)) + [\"title\"]\n",
    "cont_names = list(filter(lambda x: x not in [\"installation_id\", dep_var] + cat_names,\n",
    "                         train.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(train, path=\"/kaggle/working\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "        .split_by_idx(valid_idx=valid_idx)\n",
    "        .label_from_df(cols=dep_var, label_cls=FloatList)\n",
    "        .add_test(TabularList.from_df(test, path=home, cat_names=cat_names, cont_names=cont_names, procs=procs))\n",
    "        .databunch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy as sp\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "class OptimizedRounder():\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_coef, labels):\n",
    "        self.coef_ = 0\n",
    "        self.initial_coef = initial_coef\n",
    "        self.labels = labels\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = self.labels)\n",
    "        return -cohen_kappa_score(X_p, y, weights=\"quadratic\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = self.labels)\n",
    "\n",
    "    def coefficients(self): return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.metrics import RegMetrics\n",
    "\n",
    "class KappaScoreRegression(RegMetrics):\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        preds = self.preds.flatten()\n",
    "        opt = OptimizedRounder([0.5, 1.5, 2.0], labels=[0, 1, 2, 3])\n",
    "        opt.fit(preds, self.targs)\n",
    "        coefs = opt.coefficients()\n",
    "        def rounder(preds):\n",
    "            y = preds.clone()\n",
    "            y[y < coefs[0]] = 0\n",
    "            y[y >= coefs[2]] = 3\n",
    "            y[(y >= coefs[0]) & (y < coefs[1])] = 1\n",
    "            y[(y >= coefs[1]) & (y < coefs[2])] = 2\n",
    "            return y.type(torch.IntTensor)\n",
    "\n",
    "        qwk = cohen_kappa_score(rounder(preds), self.targs, weights=\"quadratic\")\n",
    "        return add_metrics(last_metrics, qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "\n",
    "learn = tabular_learner(data, layers=[2000,100],\n",
    "                        metrics=[KappaScoreRegression()],\n",
    "                        y_range=[0, 3],\n",
    "                        emb_drop=0.04,\n",
    "                        ps=0.6,\n",
    "                        callback_fns=[partial(EarlyStoppingCallback, monitor=\"kappa_score_regression\", mode=\"max\", patience=7),\n",
    "                                      partial(SaveModelCallback, monitor=\"kappa_score_regression\", mode=\"max\", name=\"best_model\")]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(40, 9e-03)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(40, 2.5e-03)  \n",
    "#learn.fit_one_cycle(40, 1e-02)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train, y = learn.get_preds(ds_type=DatasetType.Valid)\n",
    "labels_train = preds_train.flatten()\n",
    "opt = OptimizedRounder([0.5, 1.5, 2.0], labels=[0, 1, 2, 3])\n",
    "opt.fit(labels_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = opt.coefficients(); coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounder(preds):\n",
    "    y = preds.clone()\n",
    "    y[y < coefs[0]] = 0\n",
    "    y[y >= coefs[2]] = 3\n",
    "    y[(y >= coefs[0]) & (y < coefs[1])] = 1\n",
    "    y[(y >= coefs[1]) & (y < coefs[2])] = 2\n",
    "    return y.type(torch.IntTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y = learn.get_preds(ds_type=DatasetType.Test)\n",
    "labels = preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = rounder(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"installation_id\": test.installation_id, \"accuracy_group\": labels})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "len(submission), submission.accuracy_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
